



We have a dataset                                                             we have a base learner 1

now base learner 1 is trained on this dataset

now some observatoins from dataset(sample dataset) were tested and classified wrongly 


we have wrongly classified dataset by base learner 1                           we have a base learner 2

now base learner 2 is trained on this wrongly classified dataset

now some random observations from initial dataset were tested and wrongly classified


we have wrongly classified dataset by base learner 2                            we have a base learner 3

now base learner 3 is trained on this wrongly classified dataset by base learner 2

now some random observations from initial dataset were tested and wrongly classified


                  and so on ....................
                  
                
                
                This is known as Boosting



Adaboost Boosting

there is this dataset


f1   f2   f3   f4   o/p

a    v    g     x    1

c    d    h      b    2


f     e    f    s     1



now we add weights for all the observations in adaboost
initially
weight = 1/n  (for all observations)


Step 1:


f1   f2   f3   f4   o/p  w

a    v    g     x    1   1/3

c    d    h      b    2  1/3


f     e    f    s     1  1/3


Step 2 :


here in adaboost the base learners are decission trees of depth 1 which are also called as stumps 


for each feature each stump

now which we be the base learner 1 

so which stomp will have less entrpy will be selected as base learner 1

now we will calculate error rate for the stump

    so error rate = no of wrongly classified * (weight)  {weight = 1/3 in our usecase}
    

Step 3:


Performance of Stump = 1/2 * log((1-TE)/TE)



Step 4:


New sample weight = weight * e power(negative Performance of Stump)


f1   f2   f3   f4   o/p  w    updated weight

a    v    g     x    1   1/3    0.05

c    d    h      b    2  1/3     0.76


f     e    f    s     1  1/3     0.38

next step

f1   f2   f3   f4   o/p  w    updated weight  normalized weight

a    v    g     x    1   1/3    0.05           0.05/0.68

c    d    h      b    2  1/3     0.76             0.76/0.68


f     e    f    s     1  1/3     0.38            0.38/0.68

next step

f1   f2   f3   f4   o/p   normalized weight

a    v    g     x    1      0.05/0.68

c    d    h      b    2     0.76/0.68


f     e    f    s     1     0.38/0.68


sum of normalized weights will be 1


Step 5 :


now all the observations will divided into buckets for their normalized values like 

 bucket 1  -  0   to   0.09
 
 bucket 2  -  0.09  to  0.3
 
 bucket 3  - 0.3 to o.4     like that
 
 
 now algorithms chooses a normalized weight value and search for that observation in those buckets
 
 and these are selected as observation for training the next base leaner2 (repeating above steps)
 
 
 
 
 and finally my error rate will be decreased compared to first base learner
 
 
 
 and next for test data the voting classifier gives the output as for random forest

